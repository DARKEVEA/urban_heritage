import argparse
import json
import os
import time
from typing import Dict, Any, List, Optional, Tuple

import geopandas as gpd
import pandas as pd
import requests
from shapely.geometry import Point

from src.config import key0, key1, key2, gd_key0, gd_key1, tx_key0, tx_key1


# åæ ‡è½¬æ¢ï¼šWGS84 -> GCJ-02 -> BD-09
import math

PI = math.pi
A = 6378245.0
EE = 0.00669342162296594323

query = (
    "é“¶è¡Œ|é¢†äº‹é¦†|æ”¿åºœ|åŒ»é™¢|å…¬å®‰å±€|å¯ºåº™|"
    "ä½å®…|å…¬å¯“|é…’åº—|"
    "å¤§æ¥¼|å¹¿åœº|ç»¼åˆä½“|ä¸­å¿ƒ|"
)

queries_list = [
    "é¢†äº‹é¦†", "é“¶è¡Œ", "åŒ»é™¢", "å†™å­—æ¥¼", "å¯ºåº™", "ä½å®…", "å•†åœº", "é…’åº—"
]

query_test = ["é¢†äº‹é¦†"]

CONFIG = {
    # åˆå§‹ JSON æ–‡ä»¶è·¯å¾„
    "JSON_IN": r"E:\CODE\urban_heritage\data\processed\filtered_for_api.json",

    # å¾ªç¯æœç´¢æ—¶ JSON æ–‡ä»¶è·¯å¾„
    "OUTPUT_MATCHED": r"E:\CODE\urban_heritage\data\processed\matched.json",
    "OUTPUT_UNMATCHED": r"E:\CODE\urban_heritage\data\processed\unmatched.json",
}


def _out_of_china(lng: float, lat: float) -> bool:
    return not (72.004 <= lng <= 137.8347 and 0.8293 <= lat <= 55.8271)

def _transform_lat(lng: float, lat: float) -> float:
    ret = -100.0 + 2.0 * lng + 3.0 * lat + 0.2 * lat * lat + 0.1 * lng * lat + 0.2 * math.sqrt(abs(lng))
    ret += (20.0 * math.sin(6.0 * lng * PI) + 20.0 * math.sin(2.0 * lng * PI)) * 2.0 / 3.0
    ret += (20.0 * math.sin(lat * PI) + 40.0 * math.sin(lat / 3.0 * PI)) * 2.0 / 3.0
    ret += (160.0 * math.sin(lat / 12.0 * PI) + 320 * math.sin(lat * PI / 30.0)) * 2.0 / 3.0
    return ret

def _transform_lng(lng: float, lat: float) -> float:
    ret = 300.0 + lng + 2.0 * lat + 0.1 * lng * lng + 0.1 * lng * lat + 0.1 * math.sqrt(abs(lng))
    ret += (20.0 * math.sin(6.0 * lng * PI) + 20.0 * math.sin(2.0 * lng * PI)) * 2.0 / 3.0
    ret += (20.0 * math.sin(lng * PI) + 40.0 * math.sin(lng / 3.0 * PI)) * 2.0 / 3.0
    ret += (150.0 * math.sin(lng / 12.0 * PI) + 300.0 * math.sin(lng / 30.0 * PI)) * 2.0 / 3.0
    return ret

def wgs84_to_gcj02(lng: float, lat: float) -> Tuple[float, float]:
    if _out_of_china(lng, lat):
        return lng, lat
    dlat = _transform_lat(lng - 105.0, lat - 35.0)
    dlng = _transform_lng(lng - 105.0, lat - 35.0)
    radlat = lat / 180.0 * PI
    magic = math.sin(radlat)
    magic = 1 - EE * magic * magic
    sqrtmagic = math.sqrt(magic)
    dlat = (dlat * 180.0) / ((A * (1 - EE)) / (magic * sqrtmagic) * PI)
    dlng = (dlng * 180.0) / (A / sqrtmagic * math.cos(radlat) * PI)
    mglat = lat + dlat
    mglng = lng + dlng
    return mglng, mglat

def gcj02_to_bd09(lng: float, lat: float) -> Tuple[float, float]:
    z = math.sqrt(lng * lng + lat * lat) + 0.00002 * math.sin(lat * PI)
    theta = math.atan2(lat, lng) + 0.000003 * math.cos(lng * PI)
    bd_lng = z * math.cos(theta) + 0.0065
    bd_lat = z * math.sin(theta) + 0.006
    return bd_lng, bd_lat

def wgs84_to_bd09(lng: float, lat: float) -> Tuple[float, float]:
    lng2, lat2 = wgs84_to_gcj02(lng, lat)
    return gcj02_to_bd09(lng2, lat2)

def load_buildings(path: str):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f).get("buildings", [])

def save_buildings(path: str, buildings: List[dict]):
    with open(path, "w", encoding="utf-8") as f:
        json.dump({"buildings": buildings}, f, ensure_ascii=False, indent=2)

def load_match_json(json_path: str) -> pd.DataFrame:
    with open(json_path, "r", encoding="utf-8") as f:
        payload = json.load(f)
    rows = payload.get("buildings", [])
    return pd.DataFrame(rows)

def filter_no_name(input_path: str, output_path_filtered: str, output_path_other: str):
    # è¯»å–åŸ JSON
    with open(input_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æå– buildings åˆ—è¡¨
    buildings = data.get("buildings", [])

    # è¿‡æ»¤å‡º osm_properties ä¸­ name ä¸ºç©ºçš„è®°å½•
    filtered = []
    other = []
    for b in buildings:
        osm_props = b.get("osm_properties", {})
        name_value = osm_props.get("name")
        if not name_value or str(name_value).strip() == "":
            filtered.append(b)
        else:
            other.append(b)

    # ä¿å­˜æˆæ–°çš„ JSON æ–‡ä»¶
    with open(output_path_filtered, "w", encoding="utf-8") as f:
        json.dump({"buildings": filtered}, f, ensure_ascii=False, indent=2)
    # ä¹Ÿå¯ä»¥ä¿å­˜å…¶ä»–è®°å½•
    with open(output_path_other, "w", encoding="utf-8") as f:
        json.dump({"buildings": other}, f, ensure_ascii=False, indent=2)

    print(f"ç­›é€‰å®Œæˆï¼ŒåŸæ•°æ® {len(buildings)} æ¡ï¼Œç­›é€‰å {len(filtered)} æ¡éœ€è¿›ä¸€æ­¥å¤„ç†ï¼Œå…¶ä»– {len(other)} æ¡å·²å¦å¤–ä¿å­˜ã€‚")


def baidu_place_search_nearby(bd_lng: float, bd_lat: float, ak: str, query: str, radius: int = 50, page_size: int = 5) -> Dict[str, Any]:
    url = "https://api.map.baidu.com/place/v2/search"
    params = {
        "query": query,
        "location": f"{bd_lat},{bd_lng}",  # æ³¨æ„é¡ºåº: lat,lng
        "radius": radius,
        "output": "json",
        "ak": ak,
        "page_size": page_size,
        "coord_type": "bd09ll",
        "scope": 2
    }
    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json()

def gaode_place_search_nearby(gcj_lng: float, gcj_lat: float, key: str, keywords: str, radius: int = 50, offset: int = 5, page: int = 1) -> Dict[str, Any]:
    """
    é«˜å¾·åœ°å›¾é™„è¿‘æœç´¢ API
    æ–‡æ¡£: https://restapi.amap.com/v3/place/around
    """
    url = "https://restapi.amap.com/v3/place/around"
    params = {
        "key": key,
        "location": f"{gcj_lng},{gcj_lat}",  # æ³¨æ„é¡ºåº lng,lat
        "keywords": keywords,
        "radius": radius,
        "offset": offset,
        "page": page,
        "extensions": "all",  # è·å–æ›´å¤šä¿¡æ¯
        "output": "JSON"
    }
    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json()

def tencent_place_search_nearby(lng: float, lat: float, key: str, keyword: str, radius: int = 50, page_size: int = 10, page_index: int = 1) -> Dict[str, Any]:
    """
    è…¾è®¯åœ°å›¾å‘¨è¾¹æœç´¢ API
    æ–‡æ¡£ï¼šhttps://lbs.qq.com/service/webService/webServiceGuide/webServicePlace
    """
    url = "https://apis.map.qq.com/ws/place/v1/search"
    params = {
        "keyword": keyword,  # æœç´¢å…³é”®è¯
        "boundary": f"nearby({lat},{lng},{radius})",  # æ³¨æ„é¡ºåº lat,lng
        "key": key,
        "page_size": page_size,  # æ¯é¡µæ•°é‡
        "page_index": page_index,  # é¡µç ï¼Œä»1å¼€å§‹
        "orderby": "_distance"  # æŒ‰è·ç¦»æ’åº
    }
    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json()


def enrich_status_with_batches(
    json_in: str,
    json_out: str,
    csv_out: str,
    platform_plan: list,
    query: str,
    radius: int,
    sleep_sec: float,
    progress_file: str
):
    # è¯»å–è¿›åº¦
    if os.path.exists(progress_file):
        with open(progress_file, "r", encoding="utf-8") as f:
            progress = json.load(f)
        start_index = progress.get("last_index", 0)
    else:
        start_index = 0

    df = load_match_json(json_in)
    if df.empty:
        raise ValueError("è¾“å…¥ JSON ä¸­æœªå‘ç° buildings åˆ—è¡¨")

    total = len(df)
    print(f"æ€»è®°å½•æ•°: {total}ï¼Œä» {start_index} å¼€å§‹å¤„ç†")

    results = []
    for plan in platform_plan:
        s, e, platform, key = plan["start"], plan["end"], plan["platform"], plan["key"]

        if e >= total:
            e = total - 1

        if start_index > e:
            print(f"è·³è¿‡åŒºé—´ {s}-{e}ï¼ˆå·²å¤„ç†ï¼‰")
            continue

        # ä»æ–­ç‚¹ç»§ç»­
        batch_start = max(s, start_index)

        for i in range(batch_start, e + 1):
            row = df.iloc[i]
            osm_id = row.get("osm_id")
            osm_props = row.get("osm_properties", {})
            lon = osm_props.get("centroid_x")
            lat = osm_props.get("centroid_y")

            print(f"[{platform}] å¤„ç† {i + 1}/{total}  osm_id={osm_id}  åæ ‡=({lon}, {lat})")

            if pd.isna(lon) or pd.isna(lat):
                results.append({**row.to_dict(), "status_sources": {platform: {"ok": False, "reason": "missing coordinates"}}})
                continue

            try:
                if platform == "baidu":
                    bd_lng, bd_lat = wgs84_to_bd09(float(lon), float(lat))
                    data = baidu_place_search_nearby(bd_lng, bd_lat, key, query=query, radius=radius)
                    ok = data.get("status") == 0
                    pois = data.get("results", []) if ok else []
                    top = pois[0] if pois else None
                    status_obj = {
                        "ok": ok,
                        "raw_status": data.get("status"),
                        "raw_message": data.get("message"),
                        "top_poi": top,
                        "poi_count": len(pois),
                        "search_center": {"lng": bd_lng, "lat": bd_lat},
                        "raw_data": data  # ğŸ”¹æ–°å¢ï¼Œå®Œæ•´ä¿å­˜ API å“åº”
                    }


                elif platform == "gaode":
                    gcj_lng, gcj_lat = wgs84_to_gcj02(float(lon), float(lat))
                    data = gaode_place_search_nearby(gcj_lng, gcj_lat, key, keywords=query, radius=radius)
                    ok = data.get("status") == "1"
                    pois = data.get("pois", []) if ok else []
                    top = pois[0] if pois else None
                    status_obj = {
                        "ok": ok,
                        "raw_status": data.get("status"),
                        "raw_message": data.get("info"),
                        "top_poi": top,
                        "poi_count": len(pois),
                        "search_center": {"lng": gcj_lng, "lat": gcj_lat},
                        "raw_data": data
                    }
                else:
                    raise ValueError(f"æœªçŸ¥å¹³å°: {platform}")

            except Exception as e:
                status_obj = {"ok": False, "reason": str(e)}

            row_dict = row.to_dict()
            row_dict["status_sources"] = {platform: status_obj}
            results.append(row_dict)

            # ä¿å­˜è¿›åº¦
            with open(progress_file, "w", encoding="utf-8") as f:
                json.dump({"last_index": i}, f)

            if sleep_sec > 0:
                time.sleep(sleep_sec)

        # è¾“å‡ºåŒºé—´ CSV
        flat = []
        for r in results:
            base = {k: v for k, v in r.items() if k not in ("matched_points", "osm_properties", "status_sources")}
            plat = plan["platform"]
            info = r.get("status_sources", {}).get(plat, {})

            # æŠŠæ•´ä¸ª infoï¼ˆå«æ‰€æœ‰ POIï¼‰ä¿å­˜ä¸º JSON å­—ç¬¦ä¸²
            info_json = json.dumps(info, ensure_ascii=False)

            flat.append({
                **base,
                f"{plat}_full_info": info_json  # å…¨é‡ä¿¡æ¯
            })

        pd.DataFrame(flat).to_csv(
            csv_out,
            index=False,
            encoding="utf-8-sig",
            mode='a',
            header=not os.path.exists(csv_out)
        )


    # æœ€ç»ˆè¾“å‡º JSON
    payload = {"buildings": results}
    with open(json_out, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    # æ¸…ç©º results ä»¥å…ä¸‹ä¸ªåŒºé—´é‡å¤
    results.clear()


def search_with_queries(input_json: str, queries: List[str], output_matched: str, output_unmatched: str,
                        api="baidu", ak_or_key="", start_index=0, end_index=None):
    # è¯»å–æ‰€æœ‰æ•°æ®
    all_data = load_buildings(input_json)
    match_done = load_buildings(output_matched) if os.path.exists(output_matched) else []

    if end_index is None:
        end_index = len(all_data)

    # æœ¬æ¬¡è¦å¤„ç†çš„èŒƒå›´
    batch_data = all_data[start_index:end_index]

    matched_total = match_done
    unmatched = []  # ç”¨æ¥å­˜å‚¨æœ€ç»ˆ unmatched

    for query in queries:
        print(f"\nğŸ” å½“å‰å…³é”®è¯: {query}ï¼Œå¾…æœç´¢æ•°é‡: {len(batch_data)}")

        matched = []
        unmatched_batch = []

        for b in batch_data:
            lng = b["osm_properties"]["centroid_x"]
            lat = b["osm_properties"]["centroid_y"]

            if api == "baidu":
                result = baidu_place_search_nearby(lng, lat, ak_or_key, query)
                found = result.get("results")
            elif api == "gaode":
                result = gaode_place_search_nearby(lng, lat, ak_or_key, query)
                found = result.get("pois")
            elif api == "tencent":
                result = tencent_place_search_nearby(lng, lat, ak_or_key, query)
                found = result.get("data")
            else:
                raise ValueError(f"æœªçŸ¥çš„ API å¹³å°: {api}")

            if found:
                matched.append(b)
                matched.append(result)
            else:
                unmatched_batch.append(b)

        matched_total.extend(matched)

        # è¿™ä¸€è½® unmatched ç›´æ¥æ›¿æ¢ batch_dataï¼Œç”¨äºä¸‹ä¸ª query
        batch_data = unmatched_batch

        print(f"âœ… æœ¬è½®åŒ¹é…åˆ° {len(matched)} æ¡ï¼Œå‰©ä½™ {len(unmatched_batch)} æ¡")

        if not unmatched_batch:
            break

    # æœ€ç»ˆ unmatched = æ‰¹æ¬¡å†…æœªåŒ¹é… + æ‰¹æ¬¡å¤–æ•°æ®
    unmatched = all_data[:start_index] + all_data[end_index:] + batch_data

    save_buildings(output_matched, matched_total)
    save_buildings(output_unmatched, unmatched)
    save_buildings(input_json, unmatched)

    print(f"\nğŸ¯ æ€»åŒ¹é…: {len(matched_total)} æ¡ï¼Œæœ€ç»ˆæœªåŒ¹é…: {len(unmatched)} æ¡")



def main():
    search_with_queries(
        input_json=CONFIG["JSON_IN"],
        queries=queries_list,
        output_matched=CONFIG["OUTPUT_MATCHED"],
        output_unmatched=CONFIG["OUTPUT_UNMATCHED"],
        api="baidu",
        ak_or_key=key1,
        start_index=0,
        end_index=15
    )



if __name__ == "__main__":
    main()